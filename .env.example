# ============================================================================
# HIVE-R Environment Variables
# Copy this file to .env and fill in your values:  cp .env.example .env
# ============================================================================

# Daily LLM spend limit in USD (set to 0 to disable)
DAILY_BUDGET=50

# Budget Alert Service (set to false to disable alerts)
BUDGET_ALERTS_ENABLED=true

# Slack Webhook for budget alerts (optional, leave empty to skip Slack)
SLACK_WEBHOOK_URL=

# Required: OpenAI API Key
OPENAI_API_KEY=your-openai-api-key

# Optional: HIVE API authentication (leave empty to disable)
# When set, requests must include: Authorization: Bearer <key>
HIVE_API_KEY=

# Optional: LangSmith Tracing (recommended for observability)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-api-key
LANGCHAIN_PROJECT=hive-r

# Optional: Alternative LangSmith key for trace export
LANGSMITH_API_KEY=your-langsmith-api-key
LANGSMITH_PROJECT=hive-r

# Optional: Database
DATABASE_URL=
DATABASE_PATH=./data/hive.db

# Optional: Workspace directory for file operations
# Agents can only read/write files within this directory
HIVE_WORKSPACE=/path/to/your/project

# Optional: Design system preset
# Options: default, tailwind, shadcn, chakra, mui, radix, ant, bootstrap
HIVE_DESIGN_SYSTEM=default

# Optional: Log level
# Options: debug, info, warn, error
LOG_LEVEL=info

# Circuit Breaker (LLM API resilience)
# Failures before circuit opens (pauses requests to that model)
CB_MAX_FAILURES=3
# Milliseconds to wait before trying the model again
CB_TIMEOUT_MS=60000

# Retry (LLM API transient error handling)
# Max retry attempts before giving up (exponential backoff: 1s, 2s, 4s)
RETRY_MAX_ATTEMPTS=3
# Base delay before first retry (ms). Doubles each attempt.
RETRY_BASE_DELAY_MS=1000
# Random jitter percentage (0-100). Prevents thundering herd.
RETRY_JITTER_PERCENT=30

# Force router fallback level for debugging (0-3, leave empty for normal)
# 0=GPT-4o structured, 1=GPT-4o text, 2=Claude, 3=rule-based
# FORCE_FALLBACK_LEVEL=

# Optional: Server port
PORT=3000

# ============================================================================
# GitHub Integration (for auto-PR feature)
# ============================================================================
# Get a Personal Access Token from: https://github.com/settings/tokens
# Required scopes: 'repo'
GITHUB_TOKEN=
GITHUB_OWNER=your-username
GITHUB_REPO=your-repo
GITHUB_DEFAULT_BASE=main

# ============================================================================
# Semantic Memory — ChromaDB (Phase 15)
# ============================================================================
CHROMA_HOST=localhost
CHROMA_PORT=8000

# ============================================================================
# Billing — Stripe (Phase 15, Optional)
# ============================================================================
# Get keys from https://dashboard.stripe.com/apikeys
# STRIPE_SECRET_KEY=sk_test_...
# STRIPE_WEBHOOK_SECRET=whsec_...
# STRIPE_PRICE_FREE=price_...
# STRIPE_PRICE_PRO=price_...
# STRIPE_PRICE_TEAM=price_...
# STRIPE_PRICE_ENTERPRISE=price_...

# ============================================================================
# PRODUCTION SETTINGS (Phase 11)
# ============================================================================

# Sentry Error Monitoring
# Get your DSN from: https://sentry.io/settings/<org>/projects/<project>/keys/
SENTRY_DSN=
SENTRY_TRACES_SAMPLE_RATE=0.1
# Set to 'true' to send errors even in development
SENTRY_DEV=

# Production Mode
# Set to 'production' for stricter security headers
NODE_ENV=development

# CORS Settings
# Comma-separated list of allowed origins for production
CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# Memory Threshold (MB) for health checks
MEMORY_THRESHOLD_MB=1024

# ============================================================================
# OpenTelemetry Distributed Tracing
# ============================================================================
# Set to "true" to enable trace export (requires Jaeger or other OTLP backend)
OTEL_ENABLED=false
# OTLP HTTP endpoint (Jaeger all-in-one default)
OTEL_EXPORTER_URL=http://localhost:4318
# Sampling ratio: 1.0 = every request, 0.1 = 10% (recommended for production)
OTEL_SAMPLE_RATE=1.0
# Service name that appears in Jaeger
OTEL_SERVICE_NAME=hive-r

# ============================================================================
# Semantic Cache
# ============================================================================
# Enable/disable semantic caching of agent responses
CACHE_ENABLED=true
# Redis URL (leave empty for in-memory fallback in dev)
CACHE_REDIS_URL=redis://localhost:6379
# Cosine similarity threshold for semantic matching (0.0–1.0)
CACHE_SIMILARITY_THRESHOLD=0.95
# Default TTL in hours for agents not in the TTL map
CACHE_DEFAULT_TTL_HOURS=1

